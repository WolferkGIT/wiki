# Project proposals

## oLMpics

<img src='./img/oLMpics.png' width="300"/>

**Goal:** to study qualitative differences between modern pre-training methods

**Short description:** Apply a method from the paper oLMpics - On what Language Model Pre-training Captures \[Talmor et al., 2019] to ELMo, GPT, MASS and other pre-trained models.

Ask Vlad if you want to work on it.

## Mixup augmentation for NLP

**Short description:**
NLP augmentation is not widely used.
The main reason for this is the snobbery of NLP researchers who think that
"this augmentation ruins linguistic structure" or "this augmentation is too simple to help".
However, simple embedding-dropping augmentations hugely help language models and machine translation.
So let's apply
[mixup](https://forums.fast.ai/t/mixup-data-augmentation/22764)
and see if it helps the model to achieve better performance on language modeling and downstream tasks.

Ask Vlad if you want to work on it.
